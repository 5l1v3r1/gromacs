\newcommand{\nproc}{\mbox{$M$}}
\newcommand{\natom}{\mbox{$N$}}
\newcommand{\nx}{\mbox{$n_x$}}
\newcommand{\ny}{\mbox{$n_y$}}
\newcommand{\nz}{\mbox{$n_z$}}
\newcommand{\nsgrid}{NS grid}
\newcommand{\fftgrid}{FFT grid}
\newcommand{\dgrid}{\mbox{$\delta_{grid}$}}
\chapter{Algorithms}
\label{ch:algorithms}
\section{Introduction}
In this chapter we first give describe two general concepts used in
\gromacs:  {\em periodic boundary conditions} (\secref{pbc})
and the {\em group concept} (\secref{group}). The MD algorithm is
described in \secref{MD}: first a global form of the algorithm is
given, which is refined in subsequent subsections. The (simple) EM
(Energy Minimization) algorithm is described in \secref{EM}. Some
other algorithms for special purpose dynamics are described after
this.  In the final \secref{par} of this chapter a few principles are
given on which parallelization of {\gromacs} is based. The
parallelization is hardly visible for the user and is therefore not
treated in detail.

A few issues are of general interest. In all cases the {\em system}
must be defined, consisting of molecules. Molecules again consist of
particles  with defined interaction functions. The detailed
description of the {\em topology} of the molecules and of the {\em force
field} and the calculation of forces is given in
\chref{ff}. In the present chapter we describe
other aspects of the algorithm, such as pair list generation, update of
velocities  and positions, coupling to external temperature and
pressure,  conservation of constraints. The {\em analysis} of the data
generated by an MD simulation is treated in \chref{analysis}.


\section{\normindex{Periodic boundary conditions}}
\label{sec:pbc}
The classical way to minimize edge effects in a finite system is to
apply {\em periodic boundary conditions}. The atoms of the system to be simulated are put into a space-filling box, which is
surrounded by translated copies of itself (\figref{pbc}). 
Thus there are no
boundaries of the system; the artifact caused by unwanted
boundaries in an isolated cluster is now replaced by the artifact of
periodic conditions. If a crystal is simulated, such boundary conditions
are desired (although motions are naturally restricted to periodic
motions with wavelengths fitting into the box). If one wishes to
simulate  non-periodic systems, as liquids or solutions, the
periodicity by  itself causes errors. The errors can be evaluated by
comparing various system sizes; they are expected to be less severe than
the errors resulting from an unnatural boundary with vacuum.
\begin {figure}
\centerline{\psfig {figure=plots/pbc.eps,width=8cm,angle=270}}
\caption {Periodic boundary conditions in two dimensions.}
\label{fig:pbc}
\end {figure}

There are several possible shapes for space-filling unit cells. Some,
as the {\em truncated octahedron}~\cite{Adams79} approach a spherical
shape better than a cubic box and are therefore more economical for
studying an (approximately spherical) macromolecule in solution, since
less solvent molecules are required to fill the box given a minimum
distance between macromolecular images. However, a periodic system
based on the truncated octahedron is equivalent to a periodic system
based on a {\em triclinic} unit cell. The latter shape is the most
general space-filling unit cell; it comprises all possible
space-filling shapes~\cite{Bekker95}. Therefore {\gromacs} will in
future versions be based on the triclinic unit and will not contain
other unit cell shapes. However, in the present version
only rectangular boxes are allowed.
  
\gromacs uses periodic boundary conditions, combined
with the {\em minimum image convention:} only one - the nearest -
image of each particle is considered for short-range non-bonded
interaction terms.  For long-range electrostatic interactions this is
not always accurate enough, and \gromacs therefore also incorporates
lattice sum methods like Ewald Sum, PME and PPPM.
  
The box can be of arbitrary dimensions, but must be rectangular. An
isolated cluster of molecules can of course be simulated as well
within these restrictions by defining the periodic box size to be much
larger than the cluster size.

The minimum image convention implies that the cut-off radius used to
truncate non-bonded interactions must not exceed half the smallest box
size:
\beq
  R_c < \half min(a,b,c),
\eeq
otherwise more than one image would be within the cut-off distance of
the force. When a macromolecule, such as a protein, is studied in
solution,  this restriction does not suffice. In principle a single
solvent  molecule should not be able
to `see' both sides of the macromolecule. This means that an edge $a$
of the box must exceed the length of the macromolecule in the
direction of that edge {\em plus} two times the cut-off radius $R_c$.
It is common to compromise in this respect, and make the solvent layer
somewhat smaller in order to reduce the computational cost.

Each unit cell (cubic, rectangular or triclinic, the latter not being
implemented in \gromacs) is surrounded by 26 translated images. Thus a
particular image can always be identified by an index pointing to one
of 27 {\em translation vectors} and constructed by applying a
translation with the indexed vector (see \ssecref{forces}).

\section{The group concept}
\label{sec:group}
In the {\gromacs} MD and analysis programs one uses {\em groups} of
atoms to perform certain actions on. The maximum number of groups is
256, but every atom can only belong to four different groups, one of
each of the following kinds:
\begin{description}
\item[T-coupling group]
The \normindex{temperature coupling} parameters (reference
temperature, time constant, number of degrees of freedom, see
\ssecref{update}) can be defined for each T-coupling group
separately. For example, in a solvated macromolecule the solvent (that
tends to produce more heating by force and integration errors) can be
coupled with a shorter time constant to a bath than a macromolecule,
or a surface can be kept cooler than an adsorbing molecule. Many
different T-coupling groups may be defined.
\item[\normindex{Freeze group}]
Atoms that belong to a freeze group are kept stationary in the
dynamics. This is useful during equilibration, {\eg} to avoid that badly
placed solvent molecules will give unreasonable kicks to  protein atoms,
although the same effect can also be obtained by putting a restraining
potential on the atoms that must be protected. The freeze option can be
used on one or two coordinates of an atom, thereby freezing the atoms
in a plane or on a line. Many freeze groups can be defined.
\item[Accelerate group]
On each atom in an '\normindex{accelerate group}' an acceleration $\ve{a}^g$ will
be imposed. This is equivalent to an external force. This feature
makes it possible to drive the system into a non-equilibrium state and
enables to perform NEMD (non-equilibrium MD) to obtain transport
properties. 
\item[\normindex{Energy monitor group}]
Mutual interactions between all energy monitor
groups are compiled during
the simulation. This is done for Lennard Jones and Coulomb terms separately.
In principle up to 256 groups could be defined, but that would lead to
256$\times$256 items! Better use this concept sparingly.  
\end{description}
The use of groups in analysis programs is described in
\chref{analysis}.

\section{Molecular Dynamics}
\label{sec:MD}
A global flow scheme for MD is given in \figref{global}. Each
MD or  EM run requires as input a set of initial coordinates and -
optionally - initial velocities of all particles involved. This
chapter does not describe how these are obtained; for the setup of an
actual MD run check the online manual at {\wwwpage}.
\begin{figure}
\begin{center}
\addtolength{\fboxsep}{0.5cm}
\begin{shadowenv}[12cm]
{\large \bf THE GLOBAL MD ALGORITHM}
\rule{\textwidth}{2pt} \\
{\bf 1. Input initial conditions}\\[2ex]
Potential interaction $V$ as a function of atom positions\\
Positions $\ve{r}$ of all atoms in the system\\
Velocities $\ve{v}$ of all atoms in the system \\
$\Downarrow$\\
\rule{\textwidth}{1pt}\\
{\bf repeat 2,3,4} required number of steps:\\
\rule{\textwidth}{1pt}\\
{\bf 2. Compute forces} \\[1ex]
The force on any atom  \\[1ex]
$\ve{F}_i = - \frac{\partial V}{\partial \ve{r}_i}$ \\[1ex]
is computed by calculating the force between non-bonded atom pairs: \\
$\ve{F}_i = \sum_j \ve{F}_{ij}$ \\
plus the forces due to bonded interactions (which may depend on 1, 2,
3, or 4 atoms), plus restraining and/or external forces. \\
The potential and kinetic energies and the pressure tensor are computed. \\   
$\Downarrow$\\
{\bf 3. Update configuration} \\[1ex]
The movement of the atoms is simulated by numerically solving Newton's
equations of motion \\[1ex]
$\displaystyle
\frac {\de^2\ve{r}_i}{\de t^2} = \frac{\ve{F}_i}{m_i} $ \\
or \\
$\displaystyle
\frac{\de\ve{r}_i}{\de t} = \ve{v}_i ; \;\;
\frac{\de\ve{v}_i}{\de t} = \frac{\ve{F}_i}{m_i} $ \\[1ex]
$\Downarrow$ \\
{\bf 4.} if required: {\bf Output step} \\
write positions, velocities, energies, temperature, pressure, etc. \\
\end{shadowenv}
\caption{The global MD algorithm}
\label{fig:global}
\end{center}
\end{figure}

\subsection{Initial conditions}
\subsubsection*{Topology and force field}
The system topology, including a description of the force field, must
be loaded. These items are described in \chref{ff}.
All this information is static; it is never modified during the run.

\subsubsection*{Coordinates and velocities}
Then, before a run starts, the box size and the coordinates and
velocities of  all particles are required. The box size is determined
by three vectors (nine numbers) $\ve{b}_1, \ve{b}_2, \ve{b}_3$, which
represent the three basis vectors of the periodic box. While in the
present version of \gromacs only rectangular boxes are allowed, three
numbers  suffice, but the use of three vectors already
prepares for arbitrary triclinic boxes to be implemented in a later
version. 

If the run starts at $t=t_0$, the coordinates at $t=t_0$ must be
known. The {\em leap-frog algorithm,} used to update the time step
with $\Dt$ (see \ssecref{update}), requires that the velocities must
be known at $t=t_0 - \hDt$. If velocities are not available, the
program can generate initial atomic velocities $v_i, i=1\ldots 3N$
from a \normindex{Maxwellian distribution} (\figref{maxwell}) at a
given absolute temperature $T$:
\beq 
p(v_i) = \sqrt{\frac{m_i}{2 \pi kT}}\exp(-\frac{m_i v_i^2}{2kT})
\eeq
where $k$ is Boltzmann's constant (see \chref{defunits}).
\begin{figure}
\centerline{\psfig{figure=plots/maxwell.eps,angle=270,width=8cm}}
\caption{A Maxwellian distribution, generated from random numbers.}
\label{fig:maxwell}
\end{figure}
To accomplish this, normally distributed random numbers are generated
by adding twelve random numbers $R_k$ in the range $0 \le R_k < 1$ and
subtracting 6.0 from their sum. The result is then multiplied by the
standard deviation of the velocity distribution $\sqrt{kT/m_i}$. Since
the resulting total energy will not correspond exactly to the required
temperature $T$, a correction is made: first the center-of-mass motion
is removed and then all velocities are scaled such that the total
energy corresponds exactly to $T$ (see \eqnref{E-T}).

\subsubsection*{Center-of-mass motion}
The \normindex{center-of-mass velocity} (c.o.m.)is normally set to zero at every step. 
Normally there is no net external force acting on the system and the c.o.m.
velocity should remain constant. In practice, however, the update
algorithm develops a very slow change in the c.o.m. velocity, and thus
in the total kinetic energy of the system, 
specially when temperature coupling is used. If such changes are not
quenched, an appreciable c.o.m. motion develops eventually in long
runs, and the temperature will be significantly misinterpreted. The
same may happen due to overall rotational motion, but only when an
isolated cluster is simulated. In periodic systems with filled boxes,
the overall rotational motion is coupled to other degrees of freedom
and does not give any problems.

\subsection{Compute forces}
\label{subsec:forces}
As mentioned in \chref{ff}, internal forces are
either generated from fixed (static) lists, or from dynamics lists.
The latter concern non-bonded interactions between any pair of particles.

\subsubsection{Pair lists generation}
The non-bonded pair forces need to be calculated only for those pairs
$i,j$  for which the distance $r_{ij}$ between $i$ and the 
\normindex{nearest image} 
of  $j$ is less than a given cut-off radius $r_c$. Some of the
particle pairs that fulfill this criterion are excluded, when their
interaction is already fully accounted for by bonded interactions. {\gromacs}
employs  a {\em pair list} that contains 
those  particle pairs for which non-bonded forces must be calculated.
The  pair list contains the particle numbers and an index for the image
displacement vectors that must be applied to  obtain the nearest
image, for  all particle pairs that
have a  nearest-image distance less than \verb'rshort'. The list is
updated  every \verb'nstlist' steps, where \verb'nstlist' is typically
10 or  20. There is an option to calculate the total non-bonded force
on each  particle due to all particle in a shell around the
list-cutoff, {\em  i.e}, at a distance between \verb'rshort' and
\verb'rlong'.  This force is calculated during the pair list update
and  retained during \verb'nstlist' steps.

The vector $\rvij = \ve{r}_j - \ve{r}_i$ connecting nearest
images is  found by constructing
\bea
x_{ij} & = & x_{ij} - a*\verb'round'(x_{ij}/a) \\
y_{ij} & = & y_{ij} - b*\verb'round'(y_{ij}/b) \\
z_{ij} & = & z_{ij} - c*\verb'round'(z_{ij}/c)
\eea
where the length of the box edges are denoted by $a,b,c$, and the
function \verb'round'($x$)  delivers the integer number that is nearest
to $x$. The translation vector index is determined by the 27
combinations of the -1, 0, or +1 values of the three \verb'round'
function results (assuming that all primary particles are in the central box).

The particles will move during the simulation, and may move outside
the primary box. Before a new pair list is made up, all particles will
be reset to the primary box, which lies in the positive quadrant with
respect to an origin at $\ve{r}_0$, by applying
\bea
x_i & = & x_i - a*\verb'round'([x_i-x_0-a/2]/a) \\
y_i & = & y_i - b*\verb'round'([y_i-y_0-b/2]/b) \\
z_i & = & z_i - c*\verb'round'([z_i-z_0-c/2]/c)
\eea

\subsubsection{Image calculation on a grid.}
\label{sec:nsgrid}
{\gromacs} uses an \normindex{interaction list} for non-bonded interactions,
usually called the {\em \normindex{neighbor list}}.
This list is made every {\tt nstlist} MD steps, where {\tt nstlist} is
typically 10 MD steps. 
To make the neighbor list all particles that are close 
({\ie} within the cut-off) to a given particle must be found.
This searching, usually called neighbor searching (NS), 
involves periodic boundary conditions and 
determining the {\em image} (see \secref{pbc}).
When the cut-off is large compared to the box edge $l$ ($>$ 0.4$l$)
searching is done using an $O({\natom}^2)$ algorithm that computes
all distances and compares them to the cut-off $r_c$.
When the cut-off is smaller than 0.4$l$ in all directions (x,y and z)
searching is done using a grid, the {\nsgrid}. All particles are put on 
the {\nsgrid},
with the smallest spacing  $\ge$ $r_c/2$ in each of the directions
\footnote{In fact the cut-off is divided into sub-blocks, the number of which
can be chosen by the user. The default for this number (\dgrid) is 2, 
such that the {\nsgrid} spacing must be $\ge r_c/2$. For 
simplicity we will just use this particular choice
in the remainder of the text. However, it can be easily understood that 
if {\dgrid} = 3, we need at least 2{\dgrid} = 7 grid-cells, 
each of which has size $\ge r_c/3$}.
We have depicted the computational box, divided into {\nsgrid} cells in 
\figref{nsgrid}.
\begin{figure}
\centerline{\psfig{figure=plots/nsgrid.eps,width=6cm}}
\caption{The computational box in two dimensions, divided into {\nsgrid} cells with three particles, $i$, $j$ and $k$. Each {\nsgrid} cell is of size $\ge r_c/2$.}
\label{fig:nsgrid}
\end{figure}
In each spatial dimension, a particle $i$ has three images. For each direction
the image may be -1,0 or 1, corresponding to a translation over -1, 0 or +1
box vector. We do not search the surrounding {\nsgrid} cells for
neighbors of $i$ and then calculate the image, 
but rather construct the images first and then 
search neighbors corresponding to that image of $i$.
Since we demand that the number of {\nsgrid} cells $\ge$ 5 in each direction 
the same neighbor will not be found twice. For every particle, exactly 125 (5$^3$)
neighboring cells are searched. Therefore, the algorithm scales linear with the 
number of particles. Although the prefactor is large (125) the scaling behavior
makes the algorithm far superior over the standard $O({\natom}^2)$ 
algorithm when the number of particles exceeds a few hundred.

In the example of \figref{nsgrid} the image $t_x$ = 0 of particle
$i$ will find $j$ as a neighbor, while image $t_x$ = 1 of particle $i$
will find $k$ as a neighbor.

\subsubsection{Charge groups}
Where applicable, \normindex{neighbor searching} is carried out on the basis of
{\em  charge groups}. A \normindex{charge group} is a small set of nearby atoms
that  have net charge zero. Charge groups are defined in the molecular
topology. If the nearest image  distance between the {\em geometrical
centers} of the atoms of two charge groups is less than the cutoff
radius,  all atom pairs between the charge groups are included in the
pair list. This procedure avoids the creation of charges due to
the use  of a cut-off (when one charge of a dipole is within range and
the  other not), which can have disastrous consequences for the
behavior of  the Coulomb interaction function at distances near the
cut-off  radius. If molecular groups have full charges (ions), charge
groups  do not avoid adverse cut-off effects, and you should consider
using one of the lattice sum methods supplied by {\gromacs}~\cite{Berendsen93a}. 

If appropriately
constructed \normindex{shift function}s are used for the 
\normindex{electrostatic force}s, no
charge groups are needed. Such shift functions are implemented
in {\gromacs} (see \chref{ff}) but must be used with
care: in principle they should be combined with a lattice sum for
long-range electrostatics.

The actual neighbor search is performed on a grid. The details of the
algorithm are not relevant for the user and are not given here.

\subsubsection{Potential energy}
When forces are computed, the \normindex{potential energy} of each interaction
term is computed as well. The total potential energy is summed for
various contributions, such as Lennard Jones, Coulomb, and bonded
terms. It is also possible to compute these contributions for {\em
groups} of atoms that are separately defined (see \secref{group}).

\subsubsection{Kinetic energy and temperature}
The \normindex{temperature} is given by the total \normindex{kinetic energy} of the
$N$-particle system:
\beq
E_{kin} = \half \sum_{i=1}^N m_i v_i^2
\eeq
From this the absolute temperature $T$ can be computed using:
\beq
\half N_{df} kT = E_{kin}
\label{eqn:E-T}
\eeq
where $k$ is Boltzmann's constant and $N_{df}$ is the number of
degrees of freedom which can be computed from:
\beq
N_{df}  ~=~     3 N - N_c - 3
\eeq
Here $N_c$ is the number of {\em constraints} imposed on the system.
The additional 3 degrees of freedom must be removed because the three
center-of-mass velocities are constants of the motion, which are usually
set to zero.  This correction is small; in the current version of
{\gromacs} it is ignored.

The kinetic energy can also be written as a tensor, which is necessary
for pressure calculation in a triclinic system, or systems where shear
forces  are imposed:
\beq
{\bf E}_{kin} = \half \sum_i^N m_i \vvi \otimes \vvi
\eeq

\subsubsection{Pressure and virial}
The \normindex{pressure} 
tensor {\bf P} is calculated from the difference between 
kinetic energy $E_{kin}$ and the \normindex{virial} ${\bf \Xi}$
\beq
{\bf P} = \frac{2}{3 V} ({\bf E}_{kin}-{\bf \Xi})
\label{eqn:P}
\eeq
where $V$ is the volume of the computational box. 
The scalar pressure $P$, which can be used for pressure coupling in the case
of isotropic systems, is computed as:
\beq
P       = {\rm trace}({\bf P})/3
\eeq

The virial ${\bf \Xi}$ tensor is defined as 
\beq
{\bf \Xi} = -\half \sum_{i<j} \rvij \otimes \Fvij 
\eeq

In \secref{virial} the
implementation  in {\gromacs} of the virial computation is described.

\subsection{Update configuration}
\label{subsec:update}

The {\gromacs} MD program utilizes the so-called {\em leap-frog} 
algorithm~\cite{Hockney74} for the integration of the equations of
motion.  The \normindex{leap-frog} 
algorithm uses positions $\ve{r}$ at time $t$ and
velocities $\ve{v}$ at time $t-\hDt$; it updates positions and
velocities using the forces
$\ve{F}(t)$ determined by the positions at time $t$: 
\bea
\ve{v}(t+\hDt)  &~=~&   \ve{v}(t-\hDt)+\frac{\ve{F}(t)}{m}\Dt   \\
\ve{r}(t+\Dt)   &~=~&   \ve{r}(t)+\ve{v}(t+\hDt)\Dt
\eea
The algorithm is visualized in \figref{leapfrog}.
It is equivalent to the Verlet~\cite{Verlet67} algorithm:
\beq
\ve{r}(t+\Dt)~=~2\ve{r}(t) - \ve{r}(t-\Dt) + \frac{\ve{F}(t)}{m}\Dt^2+O(\Dt^4)
\eeq
The algorithm is of third order in $\ve{r}$ and is time-reversible.
See ref.~\cite{Berendsen86b} for the merits of this algorithm and comparison
with other time integration algorithms.
 
The \normindex{equations of motion} are modified for temperature coupling
 and pressure coupling, and extended to include the conservation of
constraints, all of which are described below.
\begin {figure}
\centerline{\psfig {figure=plots/leapfrog.eps,width=8cm}}
\caption[The Leap-Frog integration method.]{The Leap-Frog integration method. The algorithm is called
Leap-Frog  (Haasje Over), because r and v are leaping
like  frogs over each others back.}
\label{fig:leapfrog}
\end {figure}

\subsubsection{\normindex{Temperature coupling}}
For several reasons (drift during equilibration, drift as a result of
force truncation and integration errors, heating due to external or
frictional forces), it is necessary to control the temperature of the
system. \gromacs uses the {\em weak coupling} scheme~\cite{Berendsen84}
that mimics weak coupling with first-order kinetics to an external
heat bath with given temperature $T_0$. See ref~\cite{Berendsen91} for
a comparison of this temperature control method with the
Nos{\'e}-Hoover scheme~\cite{Nose84,Hoover85}. The effect of the algorithm is
that a deviation of the system temperature from $T_0$ is slowly
corrected according to
\beq
\frac{\de T}{\de t} = \frac{T_0-T}{\tau}
\label{eqn:Tcoupling}
\eeq
which means that a temperature deviation decays exponentially with a
time constant $\tau$.
This method of coupling has the advantage that the strength of the
coupling can be varied and adapted to the user requirement: for
equilibration purposes the coupling time can be taken quite short
({\eg} 0.01 ps), but for reliable equilibrium runs it can be taken much
longer ({\eg} 0.5 ps) in which case it hardly influences the
conservative dynamics.
 
The heat flow into or out of the system is effected by scaling the
velocities of each particle every step with a time-dependent factor
$\lambda$, given by
\beq 
\lambda = \left[ 1 + \frac{\Delta t}{\tau_T}
\left\{\frac{T_0}{T(t -  \hDt)} - 1 \right\} \right]^{1/2}
\label{eqn:lambda}
\eeq
The parameter $\tau_T$ is close to, but not exactly equal to the time constant
$\tau$ of the temperature coupling (\eqnref{Tcoupling}):
\beq
\tau = 2 C_V \tau_T / N_{df} k
\eeq
where $C_V$ is the total heat capacity of the system, $k$ is Boltzmann's
constant, and $N_{df}$ is the total number of degrees of freedom. The
reason that $\tau \neq \tau_T$ is that the kinetic energy change
caused by scaling the velocities is partly redistributed between
kinetic and potential energy and hence the change in temperature is
less than the scaling energy.  In practice, the ratio $\tau / \tau_T$
ranges from 1 (gas) to 2 (harmonic solid) to 3 (water). When we use
the term 'temperature coupling time constant', we mean the parameter
\normindex{$\tau_T$}.  
{\bf Note} that in practice the scaling factor $\lambda$ is limited to 
the range of 0.8 $<= \lambda <=$ 1.25, to avoid scaling by very large
numbers which may crash the simulation. In normal use, 
$\lambda$ will always be much closer to 1.0.
  
Strictly,  for computing the scaling factor the temperature $T$ is
needed at time $t$, but this is not available in the algorithm. In
practice, the temperature at the previous time step is used (as
indicated in \eqnref{lambda}), which is
perfectly all right since the coupling time constant is much longer
than one time step. The algorithm is stable up to $\tau_T \approx \Dt$.

\subsubsection*{\normindex{Pressure coupling}}
In the same spirit as the temperature coupling, the system can also be
coupled to a 'pressure bath'. 
This is accomplished~\cite{Berendsen84} by scaling
coordinates and box size every step with a parameter $\mu$, which has
the effect of a first-order kinetic relaxation of the pressure towards
a given reference pressure $P_0$:
\beq
\frac{\de P}{\de t} = \frac{P_0-P}{\tau_p}
\eeq
The scaling factor is given by
\beq
\mu = \left[ 1 + \frac{\Delta t}{\tau_p} \beta \{P(t) - P_0 \}
\right]^{1/3}
\label{eqn:mu}
\eeq
Here $\beta$ is the isothermal compressibility of the system. In general
this is not known. It suffices to take a rough estimate because the
value of $\beta$ only influences the non-critical time constant of the
pressure relaxation without affecting the average pressure itself. For
water at 1 atm and 300 K 
$\beta = 4.5 \times 10^{-10}$ Pa$^{-1} = 4.5 \times 10^{-5}$ Bar$^{-1}$,
which is $7.5 \times 10^{-4}$ MD units (see \chref{defunits}).
Most other liquids have similar values.

In the present version of \gromacs the pressure coupling can be done
anisotropically: the $x,y,z$ dimensions are scaled separately, based
on the diagonal elements of the pressure tensor. This allows {\eg}  to couple
one dimension to an external pressure, while keeping a fixed surface
area in the other two dimensions (useful in membrane simulations). The
system  axes remain orthogonal (the scaling method allows in principle
also  dynamic changes in box angles, but this is not implemented yet).

Since the pressure fluctuates heavily, it is recommended to take
$\tau_p$ not too small; a value between 0.4 and 1 ps will often be
satisfactory. When using lattice sum methods it is easy to get pressure
oscillations, but this can be overcome by either slower scaling or by
averaging the calculated pressure over several steps.

\subsubsection*{\normindex{Surface tension coupling}}
When a periodic system consists of more than one phase, separated by
surfaces which are parallel to the xy-plane,
the surface tension and the z-component of the pressure can be coupled
to a pressure bath.
The average surface tension $\gamma(t)$ can be calculated from
the difference between the normal and the lateral pressure:
\begin{eqnarray}
\gamma(t) & = & 
\frac{1}{n} \int_0^{L_z}
\left\{ P_z(z,t) - \frac{P_x(z,t) + P_y(z,t)}{2} \right\} \mbox{d}z \\
& = &
\frac{L_z}{n} \left\{ P_z(t) - \frac{P_x(t) + P_y(t)}{2} \right\}
\end{eqnarray}
where $L_z$ is the height of the box and $n$ is the number of surfaces.
The pressure in the z-direction is corrected by scaling the height of
the box with $\mu_z$:
\beq
\Delta P_z = \frac{\Delta t}{\tau_p} \{ P_{z0} - P_z(t) \}
\eeq
\beq
\mu_z = 1 + \beta_z \Delta P_z
\eeq
This is similar to normal pressure coupling, except that the power
of one third is missing. 
The pressure correction in the z-direction is then used to get the
correct convergence for the surface tension to the reference value $\gamma_0$.
The correction factor for the box-length in the x/y-direction is:
\beq
\mu_{xy} = \left[ 1 + \frac{\Delta t}{\tau_p} \beta_{xy}
        \left( \frac{n \gamma_0}{\mu_z L_z} 
        - \left\{ P_z(t)+\Delta P_z - \frac{P_x(t) + P_y(t)}{2} \right\} 
        \right) \right]^\frac{1}{2}
\eeq
The value of $\beta_z$ is more critical than with normal pressure
coupling. Normally an incorrect compressibility will just scale $\tau_p$,
but with surface tension coupling it affects the convergence of the surface
tension. 
When $\beta_z$ is set to zero (constant box height), $\Delta P_z$ is also set
to zero, which is necessary for obtaining the correct surface tension. 

\subsubsection*{The complete update algorithm}
The complete algorithm for the update of velocities and coordinates is
given in \figref{complete-update}. The SHAKE algorithm of step
4 is explained below. 
\begin{figure}
\begin{center}
\addtolength{\fboxsep}{0.5cm}
\begin{shadowenv}[12cm]
{\large \bf THE UPDATE ALGORITHM}
\rule{\textwidth}{2pt} \\
Given:\\
Positions $\ve{r}$ of all atoms at time $t$ \\
Velocities $\ve{v}$ of all atoms at time $t-\hDt$ \\
Accelerations $\ve{F}/m$ on all atoms at time $t$.\\
(Forces are computed disregarding any constraints)\\
Total kinetic energy and virial \\
$\Downarrow$ \\
{\bf 1.} Compute the scaling factors $\lambda$ and $\mu$\\
according to \eqnsref{lambda}{mu}\\   
$\Downarrow$ \\
{\bf 2.} Update and scale velocities: $\ve{v}' =  \lambda (\ve{v} +
\ve{a} \Delta t)$ \\
$\Downarrow$ \\
{\bf 3.} Compute new unconstrained coordinates: $\ve{r}' = \ve{r} + \ve{v}'
\Delta t$ \\
$\Downarrow$ \\
{\bf 4.} Apply constraint algorithm to coordinates: constrain($\ve{r}^{'} \rightarrow  \ve{r}'';
\,  \ve{r}$) \\
$\Downarrow$ \\
{\bf 5.} Correct velocities for constraints: $\ve{v} = (\ve{r}'' -
\ve{r}) / \Delta t$ \\
$\Downarrow$ \\
{\bf 6.} Scale coordinates and box: $\ve{r} = \mu \ve{r}''; \ve{b} =
\mu  \ve{b}$ \\
\end{shadowenv}
\caption{The MD update algorithm}
\label{fig:complete-update}
\end{center}
\end{figure}

\gromacs has a provision to ''freeze''  (prevent motion of) selected
particles, which must be defined as a 'freeze group'. This is implemented
using a {\em freeze factor $\ve{f}_g$}, which is a vector, and differs for each
{\em freezegroup} (see \secref{group}). This vector contains only
zero (freeze) or one (don't freeze).
When we take this freeze factor and the external acceleration $\ve{a}_h$ into 
account the update algorithm for the velocities becomes:
\beq
\ve{v}(t+\hdt)~=~\ve{f}_g * \lambda * \left[ \ve{v}(t-\hdt) +\frac{\ve{F}(t)}{m}\Delta t + \ve{a}_h \Delta t \right]
\eeq
where $g$ and $h$ are group indices which differ per atom.

\subsection{Constraint algorithms}

\subsubsection*{SHAKE}
\normindex{Constraints} can be imposed in {\gromacs} using the traditional 
\normindex{SHAKE}
method~\cite{Ryckaert77}. The SHAKE routine changes a set of unconstrained
coordinates $\ve{r}^{'}$ to a set of coordinates $\ve{r}''$ that
fulfill a  list of distance constraints, using a set $\ve{r}$ as
reference: \\[1ex] 
\hspace*{5em} SHAKE($\ve{r}^{'} \rightarrow  \ve{r}'';\,  \ve{r}$) \\[1ex]
This action is consistent with solving a set of Lagrange multipliers
in the constrained equations of motion. SHAKE needs a {\em tolerance}
\verb'TOL'; it will continue until all constraints are satisfied
within a {\em relative} tolerance \verb'TOL'. An error message is
given if SHAKE cannot reset the coordinates because the deviation is
too large, or if a given number of iterations is surpassed. 

Assume the equations of motion must fulfill $K$ holonomic constraints,
expressed as
\beq
\sigma_k(\ve{r}_1 \ldots \ve{r}_N) = 0; \;\; k=1 \ldots K
\eeq
({\eg} $(\ve{r}_1 - \ve{r}_2)^2 - b^2 = 0$). 
Then the forces are defined as 
\beq
- \frac{\partial}{\partial \ve{r}_i} \left( V + \sum_{k=1}^K \lambda_k
\sigma_k \right)
\eeq
where $\lambda_k$ are Lagrange multipliers which must be solved to
fulfill the constraint equations. The second part of this sum
determines the {\em constraint forces} $\ve{G}_i$, defined by
\beq
\ve{G}_i = -\sum_{k=1}^K \lambda_k \frac{\partial \sigma_k}{\partial
\ve{r}_i}
\eeq
The displacement due to the constraint forces in the leap frog or
Verlet algorithm is equal to $(\ve{G}_i/m_i)(\Dt)^2$. Solving the
Lagrange multipliers (and hence the displacements) requires the
solution of a set of coupled equations of the second degree. These are
solved iteratively by SHAKE.
For the special case of rigid water molecules, that often make up more
than 80\% of the simulation system we have implemented the 
\normindex{SETTLE}
algorithm~\cite{Miyamoto92} (\secref{settle}).


\newcommand{\fs}[1]{\begin{equation} \label{eqn:#1}}
\newcommand{\fe}{\end{equation}}
\newcommand{\p}{\partial}
\newcommand{\Bm}{\ve{B}}
\newcommand{\M}{\ve{M}}
\newcommand{\iM}{\M^{-1}}
\newcommand{\Tm}{\ve{T}}
\newcommand{\Sm}{\ve{S}}
\newcommand{\fo}{\ve{f}}
\newcommand{\con}{\ve{g}}
\newcommand{\lenc}{\ve{d}}

\subsubsection*{The LINCS algorithm}
\normindex{LINCS} is an algorithm that resets bonds to their correct lengths
after an unconstrained update~\cite{Hess97}. 
The method is non-iterative, as it always uses two steps.
Although LINCS is based on matrices, no matrix-matrix multiplications are 
needed. The method is more stable and faster than SHAKE, 
but it can only be used with bond \normindex{constraints} and 
isolated angle constraints, such as the proton angle in OH. 
Because of its stability LINCS is especially useful for Langevin Dynamics. 
LINCS has two parameters, which are explained in the subsection parameters.
 
\subsubsection*{The LINCS formulas}
We consider a system of $N$ particles, with positions given by a
$3N$ vector $\ve{r}(t)$.
For Molecular Dynamics the equations of motion are given by Newton's law
\fs{c1}
{\de^2 \ve{r} \over \de t^2} = \iM \ve{F}
\fe
where $\ve{F}$ is the $3N$ force vector 
and $\M$ is a $3N \times 3N$ diagonal matrix,
containing the masses of the particles.
The system is constrained by $K$ time-independent constraint equations
\fs{c2}
g_i(\ve{r}) = | \ve{r}_{i_1}-\ve{r}_{i_2} | - d_i = 0 ~~~~~~i=1,\ldots,K
\fe

In a numerical integration scheme LINCS is applied after an
unconstrained update, just like SHAKE. The algorithm works in two
steps (see figure \figref{lincs}). In the first step the projections
of the new bonds on the old bonds are set to zero. In the second step
a correction is applied for the lengthening of the bonds due to
rotation. The numerics for the first step and the second step are very
similar. A complete derivation of the algorithm can be found in
\cite{Hess97}. Only a short description of the first step is given
here.

\begin{figure}
\centerline{\psfig{figure=plots/lincs.eps,height=50mm}}
\caption[Schematic picture showing the three position updates needed 
for one time step.]{Schematic picture showing the three position
updates needed for one time step. The dashed line is the old bond of
length $d$, the solid lines are the new bonds. $l=d \cos \theta$ and
$p=(2 d^2 - l^2)^{1 \over 2}$.}
\label{fig:lincs}
\end{figure}

A new notation is introduced for the gradient matrix of the constraint 
equations which appears on the right hand side of the equation
\fs{c3}
B_{hi} = {\p g_h \over \p r_i}
\fe
Notice that $\Bm$ is a $K \times 3N$ matrix, it contains the directions
of the constraints.
The following equation shows how the new constrained coordinates 
$\ve{r}_{n+1}$ are related to the unconstrained coordinates
$\ve{r}_{n+1}^{unc}$
\fs{m0}
\begin{array}{c}
  \ve{r}_{n+1}=(\ve{I}-\Tm_n \ve{B}_n) \ve{r}_{n+1}^{unc} + \Tm_n \lenc=  
  \\[2mm]
  \ve{r}_{n+1}^{unc} - 
\iM \Bm_n (\Bm_n \iM \Bm_n^T)^{-1} (\Bm_n \ve{r}_{n+1}^{unc} - \lenc) 
\end{array}
\fe
where $\Tm = \iM \Bm^T (\Bm \iM \Bm^T)^{-1}$.
The derivation of this equation from \eqnsref{c1}{c2} can be found
in \cite{Hess97}.

This first step does not set the real bond lengths to the prescribed lengths,
but the projection of the new bonds onto the old directions of the bonds.
To correct for the rotation of bond $i$, the projection of the
bond on the old direction is set to 
\fs{m1a}
p_i=\sqrt{2 d_i^2 - l_i^2}
\fe
where $l_i$ is the bond length after the first projection.
The corrected positions are 
\fs{m1b}
\ve{r}_{n+1}^*=(\ve{I}-\Tm_n \Bm_n)\ve{r}_{n+1} + \Tm_n \ve{p} 
\fe
This correction for rotational effects is actually an iterative process,
but during MD only one iteration is applied.
The relative constraint deviation after this procedure will be less than
0.0001 for every constraint.
In energy minimization this might not be accurate enough, so the number
of iterations is equal to the order of the expansion (see below).

Half of the CPU time goes to inverting the constraint coupling 
matrix $\Bm_n \iM \Bm_n^T$, which has to be done every time step.
This $K \times K$ matrix
has $1/m_{i_1} + 1/m_{i_2}$ on the diagonal.
The off-diagonal elements are only non-zero when two bonds are connected,
then the element is 
$\cos \phi /m_c$,  where $m_c$ is 
the mass of the atom connecting the
two bonds and $\phi$ is the angle between the bonds.

The matrix $\Tm$ is inverted through a power expansion.
A $K \times K$ matrix $\ve{S}$ is 
introduced which is the inverse square root of 
the diagonal of $\Bm_n \iM \Bm_n^T$.
This matrix is used to convert the diagonal elements 
of the coupling matrix to one
\fs{m2}
\begin{array}{c}
(\Bm_n \iM \Bm_n^T)^{-1}
= \Sm \Sm^{-1} (\Bm_n \iM \Bm_n^T)^{-1} \Sm^{-1} \Sm  \\[2mm]
= \Sm (\Sm \Bm_n \iM \Bm_n^T \Sm)^{-1} \Sm =
  \Sm (\ve{I} - \ve{A}_n)^{-1} \Sm
\end{array}
\fe
The matrix $\ve{A}_n$ is symmetric and sparse and has zeros on the diagonal.
Thus a simple trick can be used to calculate the inverse
\fs{m3}
(\ve{I}-\ve{A}_n)^{-1}= 
        \ve{I} + \ve{A}_n + \ve{A}_n^2 + \ve{A}_n^3 + \ldots
\fe

This inversion method is only valid if the absolute values of all the
eigenvalues of $\ve{A}_n$ are smaller than one.
In molecules with only bond constraints the connectivity is so low
that this will always be true, even if ring structures are present.
Problems can arise in angle-constrained molecules.
By constraining angles with additional distance constraints
multiple small ring structures are introduced.
This gives a high connectivity, leading to large eigenvalues.
Therefore LINCS should NOT be used with coupled angle-constraints.

\subsubsection*{The LINCS Parameters}
The accuracy of LINCS depends on the number of matrices used
in the expansion \eqnref{m3}. For MD calculations a fourth order
expansion is enough. For Position Langevin Dynamics with
large time steps an eighth order expansion may be necessary.
The order is a parameter in the input file for \verb'mdrun'.
The implementation of LINCS is done in such a way that the 
algorithm will never crash. Even when it is impossible to
to reset the constraints LINCS will generate a conformation
which fulfills the constraints as well as possible.
However, LINCS will generate a warning when in one step a bond 
rotates over more than a predefined angle.
This angle is set by the user in the input file for \verb'mdrun'.


\subsection{Output step}
The important output of the MD run is the {\em
\normindex{trajectory file}} \verb'name.trj' which contains particle coordinates
and -optionally- velocities at regular intervals. Since the trajectory
files are lengthy, one should not save every step! To retain all
information it suffices to write a frame every 15 steps, since at
least 30 steps are made per period of the highest frequency in the
system, and Shannon's \normindex{sampling} theorem states that two samples per
period of the highest frequency in a band-limited signal contain all
available information. But that still gives very long files! So, if
the highest frequencies are not of interest, 10 or 20 samples per ps
may suffice. Be aware of the distortion of high-frequency motions by
the {\em stroboscopic effect}, called {\em aliasing}: higher frequencies
are  mirrored with respect to the sampling frequency and appear as
lower frequencies. 

A more detailed flow sheet of the MD algorithm is given in
\figref{flowdetail}. 

\begin{figure}
\begin{center}
\addtolength{\fboxsep}{0.5cm}
\begin{shadowenv}[12cm]
{\large \bf THE MD ALGORITHM}
\rule{\textwidth}{2pt} \\
{\bf Input initial conditions} \\[2ex]
Potential interaction $V$ as a function of atom positions \\
Positions $\ve{r}$ of all atoms at time $t$ \\
(Velocities $\ve{v}$ of all atoms at time $t-\hDt$) \\
$\Downarrow$ \\
(if required:) Generate initial velocities; reset c.o.m. velocity to
zero; correct velocities for exact temperature \\
$\Downarrow$ \\
{\bf repeat} required number of blocks of $n$ steps:
\begin{shadowenv}[10cm]
reset particles into primary box \\
$\Downarrow$ \\
generate pair list and compute shell forces\\
$\Downarrow$ \\
{\bf repeat} $n$ times:
\begin{shadowenv}[8cm]
reset c.o.m. velocities \\
$\Downarrow$ \\
update configuration \\
(if required:) output step
\end{shadowenv}
\end{shadowenv}
\end{shadowenv}
\caption{the MD algorithm}
\label{fig:flowdetail}
\end{center}  
\end{figure}

\section{Simulated Annealing}
\label{sec:SA}
The well known \normindex{simulated annealing}
(SA) protocol is implemented
in a simple way into {\gromacs}. A modification of the temperature coupling
scheme is used as a very basic implementation of the SA algorithm. The
method works as follows: the reference temperature for coupling $T_0$
(\eqnref{Tcoupling})
is not constant but can be varied linearly:
\beq
T_0({\rm step}) = T_0 * (\lambda_0 + \Delta\lambda * {\rm step})
\label{eqn:SA}
\eeq
if $\lambda_0$ = 1 and $\Delta\lambda$ is 0 this is the plain MD
algorithm. Note that for standard SA $\Delta\lambda$  must be negative.
When $T_0$(step) $<$ 0 it is set to 0, as negative temperatures do not have
a physical meaning. This ``feature'' 
allows for an annealing strategy in which
at first the temperature is scaled down linearly until 0 K, 
and when more steps
are taken the simulation proceeds at 0 K. Since the weak coupling scheme
does not couple instantaneously, the actual temperature will
always be slightly higher than 0 K.

\section{Langevin Dynamics}
\newcommand{\vrond}{\stackrel{\circ}{\ve{r}}}
\newcommand{\rond}{\stackrel{\circ}{r}}
\newcommand{\ruis}{\ve{r}^G}
\label{sec:LD}
The Position Langevin Dynamics algorithm is implemented in {\gromacs} is 
(note: NOT {\em Velocity} Langevin Dynamics). 
This applies to over-damped systems, 
{\ie} systems in which the inertia effects are negligible.
The equations are
\beq
{\de \ve{r} \over \de t} = {\ve{F}(\ve{r}) \over \gamma} + \vrond
\eeq 
where $\gamma$ is the friction coefficient $[\mbox{amu/ps}]$ and
$\vrond(t)$  is a noise process with 
$\langle \rond_i\!(t) \rond_j\!(0) \rangle = 
    2 \delta(t) \delta_{ij} k_b T / \gamma$.
In {\gromacs} the equations are integrated with an explicit scheme
\beq
\ve{r}_{n+1} = \ve{r}_{n} +
        {\Delta t \over \gamma} \ve{F}(\ve{r}_n) 
        + \sqrt{2 k_b T {\Delta t \over \gamma}}\, \ruis 
\eeq
where $\ruis$ is  Gaussian distributed noise with $\mu = 0$, $\sigma = 1$.
Because the system is assumed to be over damped, large time-steps
can be used. LINCS should be used for the constraints since SHAKE
will not converge for large atomic displacements.
LD is an option of the \verb'mdrun' program.

\section{Energy Minimization}
\label{sec:EM}
Energy minimization in {\gromacs} can be done using a  simple 
\normindex{steepest descent} method. 
\normindex{Conjugate gradient} minimization is planned for the
next release. EM is just an option of the \verb'mdrun' program.

\subsection{Steepest Descents.}
Although Steepest Descents is certainly not the most efficient
algorithm for searching, it is robust and easy to implement.
It can not be used with SHAKE, so explicit bond interactions must be
specified in the topology.

We define the vector $\ve{r}$ as the vector of all $3N$ coordinates.
Initially a maximum displacement $h_0$ ({\eg} 0.01 nm) must be given. 

First the forces $\ve{F}$ and potential energy are calculated.
New positions are calculated by
\beq
\ve{r}_{n+1} =  \ve{r}_n + \frac{\ve{F}_n}{\max (|\ve{F}_n|)} h_n
\eeq
where $h_n$ is the maximum displacement and $\ve{F}_n$ is the force,
or the negative gradient of the  potential $V$. The notation $\max
(|\ve{F}_n|)$ means the largest of the absolute values of the force
components.  The forces and energy are again computed for the new positions \\
If ($V_{n+1} < V_n$) the new positions are accepted and $h_{n+1} = 1.2
h_n$. \\
If ($V_{n+1} \geq V_n$) the new positions are rejected and $h_n = 0.2 h_n$.

The algorithm stops when either a user specified number of force 
evaluations has been performed ({\eg} 100), or when the maximum of the absolute
values of the force (gradient) components is smaller than a specified
value $\epsilon$.
Since force truncation produces some noise in the
energy evaluation, the stopping criterion should not be made too tight
to avoid endless iterations. A reasonable value for $\epsilon$ can be
estimated from the root mean square force $f$ a harmonic oscillator would exhibit at a
temperature $T$ This value is 
\beq
  f = 2 \pi \nu \sqrt{ 2mkT}
\eeq
where $\nu$ is the oscillator frequency, $m$ the (reduced) mass, and
$k$ Boltzmann's constant. For a weak oscillator with a wave number of
100 cm$^{-1}$ and a mass of 10 atomic units, at a temperature of 1 K,
$f=7.7$ kJ~mol$^{-1}$~nm$^{-1}$. A value for $\epsilon$ between 1 and
10 is acceptable.   

\section{Normal Mode Analysis}
\normindex{Normal mode analysis}~\cite{Levitt83,Go83,BBrooks83b} 
can be performed using {\gromacs}, by diagonalization of the mass-weighted
\normindex{Hessian}:
\beq
M^{-1/2} H M^{-1/2} Q   ~=~     \omega^2 Q
\eeq
where $M$ contains the atomic masses, $Q$ contains eigenvectors, and $\omega$
contains the corresponding eigenvalues (frequencies).

First, the Hessian matrix, which is a 3N x 3N matrix where N is the number
of atoms, has to be calculated:
\beq
H_{ij}  ~=~     \frac{\partial^2 V}{\partial x_i \partial x_j}
\eeq
where $x_i$ and $x_j$ denote the atomic x,y or z coordinates.
In practice, these equations have not been developed analytically, but
the force is used
\beq
F_i     ~=~     \frac{\partial V}{\partial x_i}
\eeq
from which the Hessian is computed numerically. It should be noted that
for a usual Normal Mode calculation, it is necessary to completely minimize 
the energy prior to computation
of the Hessian. A number of {\gromacs} programs are involved in these
calculations. First \normindex{nmrun}, which computes the Hessian,
and secondly \normindex{g\_nmeig} which does the diagonalization and
sorting of normal modes according to frequencies. Both these programs
should be run in double precision. An overview of normal mode analysis
and the related \normindex{essential dynamics} or
\normindex{principal component analysis} can be found in ~\cite{Hayward95b}.

\section{Free energy perturbation}
\label{sec:fepalg}
\normindex{Free energy perturbation} calculations can be performed
in {\gromacs} using slow-growth methods. An example problem might be:
calculate the difference in free energy of binding of an inhibitor {\bf I}
to an enzyme {\bf E} and to a mutated enzyme {\bf E'}.
It is not feasible with computer simulations to perform a docking
calculation for such a large complex, or even releasing the inhibitor from
the enzyme in a reasonable amount of computer time with reasonable accuracy.
However, if we consider the free energy cycle in (\figref{free}A)
we can write
\beq
\Delta G_1 - \Delta G_2 =       \Delta G_3 - \Delta G_4
\label{eqn:ddg}
\eeq
If we are interested in the left-hand term we can equally well compute
the right-hand term.
\begin{figure}
\centerline{\psfig{figure=plots/free1.eps,width=6cm,angle=270}\hspace{2cm}\psfig{figure=plots/free2.eps,width=6cm,angle=270}}
\caption[Free energy cycles.]{Free energy cycles. {\bf A:} to
calculate $\Delta G_{12}$ or the free energy difference between the
binding of inhibitor {\bf I} to enzymes {\bf E} respectively {\bf
E'}. {\bf B:} to calculate $\Delta G_{12}$ which is the free energy
difference for binding of inhibitors {\bf I} respectively {\bf I'} to
enzyme {\bf E}.}
\label{fig:free}
\end{figure}

If we want to compute the difference in free energy of binding of two
inhibitors {\bf I} and {\bf I'} to an enzyme {\bf E} (\figref{free}B)
we can again use \eqnref{ddg} to compute the desired property.

\section{Essential Dynamics Sampling}
The results from an Essential Dynamics (ED) analysis \cite{Amadei93}
of a protein can be used to guide MD simulations. The idea is that
from an initial MD simulation (or from other sources) a definition of
the collective fluctuations with largest amplitude is obtained. The
position along one or more of these collective modes can be
constrained in a (second) MD simulation in a number of ways for
several purposes. For example, the position along a certain mode may
be kept fixed to monitor the average force (free-energy gradient) on
that coordinate in that position. Another application is to enhance
sampling efficiency with respect to usual MD
\cite{Degroot96a,Degroot96b}. In this case, the system is encouraged
to sample its available configuration space more systematically than
in a diffusion-like path that proteins usually take.

All available constraint types are described in the appropriate chapter
of the WHAT IF \cite{Whatif} manual.


\section{Parallelization}
\label{sec:par}

\newcommand{\abs}[1]{\mid \! {#1} \! \mid}

The purpose of this section is to discuss the 
\normindex{parallelization} of the 
principle MD algorithm and not to describe the algorithms that are in 
practical use for molecular systems with their complex variety of atoms 
and terms in the force field descriptions. We shall therefore consider 
as an example a simple system consisting only of a single type of atoms 
with a simple form of the interaction potential. The emphasis will be 
on the special problems that arise when the algorithm is implemented on 
a parallel computer. 

The simple model problem already contains the bottleneck of all MD 
simulations: the computationally intensive evaluation of the 
{\em non-bonded} forces between pairs of atoms, based on the distance 
between particles. Complex molecular systems will in addition 
involve many different kinds of {\em bonded} forces between designated 
atoms. Such interactions add to the complexity of the algorithm but do 
not modify the basic considerations concerning parallelization.


\subsection{Methods of parallelization}
There are a number of methods to parallelize the MD algorithm, each of
them with their own advantages and disadvantages. The method to 
choose depends on the hardware and compilers available.
We list them here:
\begin{enumerate}
\item[1]        {\em \normindex{Message Passing}.}\\
                In this method, which is more or less the traditional
                way of parallel programming, all the parallelism is
                explicitly programmed by the user. The disadvantage
                is that it takes extra code and effort, the advantage
                is that the programmer keeps full control over the data
                flow and can do optimizations a compiler could not come 
                up with. 

                The implementation is typically done by calling a set of 
                library routines to send and receive data to and from 
                other processors. Almost all hardware vendors support 
                this way of
                parallelism in their C and Fortran compilers.
                
\item[2]        {\em \normindex{Data Parallel}.}\\
                This method lets the user define arrays on which to
                operate in parallel. Programming this way is much
                like vectorizing: recurrence is not parallelized
                ({\eg} {\tt for(i=1; (i<MAX); i++) a[i] = a[i-1] + 1;}
                does not vectorise and not parallelize, because for
                every i the result from the previous step is needed).

                The advantage of data parallelism is that it is
                easier for the user; the compiler takes care of the
                parallelism. The disadvantage is that it is supported
                by a small (though growing) number of hardware vendors,
                and that it is much harder to maintain a program that has to
                run on both parallel and sequential machines, because
                the only standard language that supports it is Fortran-90
                which is not available on many platforms.
\end{enumerate}
Both methods allow for the MD algorithm to be implemented without much
trouble. Message passing MD algorithms have been published
since the mid 80's (\cite{Fincham87}, \cite{Raine89}) 
and development is still continuing. 
Data parallel programming is newer,
but starting from a well vectorized program it is not hard to do.

Our implementation of MD is a message passing one, the reason for which
is partly historical: the project to develop a parallel MD program started
when Fortran-90 was still in the making, and no compilers were
expected to be available. 
At current, we still believe that message passing is the way
to go, after having done some experiments with data parallel programming on a
Connection Machine (CM-5), because of portability to other hardware,
the poor performance of the code produced by the compilers 
and because this way of programming
has the same drawback as vectorization: the part of the program that is
not vectorized or parallelized determines the runtime of the program
(\normindex{Amdahl's law}).

The approach we took to parallelism was a minimalist one: use as little
non-standard elements in the software as possible, and use the
simplest \normindex{processor topology} that does the job. We therefore 
decided to use a standard language (ANSI-C) with as little non-standard
routines as possible. We only use 5 communication routines that are
non-standard. It is therefore very easy to port our code to other machines.

For an $O(N^2)$ problem like MD, one of the best schemes for the
interprocessor connections is a ring, so our software demands that a
ring is present in the interprocessor connections. A ring can almost
always be mapped onto another network like a \normindex{hypercube}, a
bus interface (Ethernet {\eg} using \seeindex{Parallel Virtual
Machines}{PVM} \normindex{PVM}~\cite{pvm3}) or a \normindex{tree}
(CM-5). Some hardware vendors have very luxurious connection schemes
that connect every processor to every other processor, but we do not
really need it and so do not use it even though it might come in handy
at times.

When using a message passing scheme one has to divide the particles 
over processors, which can be done in two ways:
\begin{itemize}
\item   {\em \normindex{Space Decomposition}.}\\
        An element of space is allocated to each processor, when dividing
        a cubic box with edge $b$ over $P$ processors this can be done 
        by giving
        each processor a slab of length $b/P$. This method 
        has the advantage
        that each processor has about the same number of interactions
        to calculate (at least when the simulated system has a homogeneous
        density, like a liquid or a gas). The disadvantage is that a lot of
        bookkeeping is necessary for particles that move over processor
        boundaries. When using more complex systems like macromolecules there
        are also 3- and 4-atom interactions that would 
        complicate the bookkeeping so much that this method is not used
        in our program.
\item   {\em \normindex{Particle Decomposition}.}\\
        Every processor is allocated a number of particles. When
        dividing $N$ particles over $P$ processors each processor will
        get $N/P$ particles. The implementation of this method
        is described in the next section.
\end{itemize}

\subsection{MD on a ring of processors}
When a neighbor list is not used the MD problem is in principle an $O(N^2)$ 
problem as each particle can interact
with every other. This can be simplified using Newton's third law
\beq
F_{ij}  ~=~     -F_{ji}
\label{eqn:Newt3}
\eeq
This implies that there is half a matrix of interactions (without diagonal, 
a particle does not interact with itself) to consider 
(\figref{int_mat}).
\begin {figure}
\centerline{\psfig{figure=plots/int_mat.eps,width=10cm}}
\caption {The interaction matrix (left) and the same using action=-reaction (right).}
\label{fig:int_mat}
\end {figure}
When we reflect the upper right triangle of interactions to the lower
left triangle of the matrix, we still cover all possible interactions,
but now every row in the matrix has almost the same number of points
or possible interactions.  We can now assign a (preferably equal)
number of rows to each processor to compute the forces and at the same
time a number of particles to do the update on, the {\em home}
particles. The number of interactions per particle is dependent on the
{\em total number} $N$ of particles (see \figref{decomp}) and on the
{\em particle number} $i$.  The exact formulae are given in
\tabref{decomp}
\begin{table}
\caption[The number of $j$ particles per $i$ particle is a function of
the total number of particles $N$ and particle number $i$.]{The number
of $j$ particles per $i$ particle is a function of the total number of
particles $N$ and particle number $i$. Note that the / operator
implies integer division, {\ie} with truncation.}
\captspace
\label{tab:decomp}
\centerline{
\begin{tabular}{l|c|c|c|c}
\dline
           & i mod 2 = 0 & i mod 2 = 0  & i mod 2 = 1   & i mod 2 = 1 \T \\
           & i $<$ N/2& i $\ge$ N/2     & i $<$ N/2     & i $\ge$ N/2 \B \\
\hline
N mod 2 = 1 & N/2       & N/2           & N/2           & N/2   \T      \\
N mod 4 = 2 & N/2       & N/2           & N/2 - 1       & N/2 - 1       \\
N mod 4 = 0 & N/2       & N/2 - 1       & N/2 - 1       & N/2   \B      \\
\dline
\end{tabular}
}
\end{table}

\begin {figure}
\centerline{\psfig {figure=plots/decomp.eps,width=\linewidth}}
\caption[Interaction matrices for different $N$.]{Interaction matrices for different $N$. The number of $j$-particles an $i$-particle interacts with depends on the {\em total} number of particles and on the {\em particle number}.}
\label{fig:decomp}
\end {figure}

A flow chart of the algorithm is given in \figref{mdpar}.
\begin {figure}
\centerline{\psfig {figure=plots/mdpar.eps,width=6cm}}
\caption {The (Parallel) MD algorithm. If the steps marked * are left out we have the sequential algorithm again.}
\label{fig:mdpar}
\end {figure}
It is the same as the sequential algorithm, except for two communication
steps. After the particles have been reset in the box, each processor
sends its coordinates left and then starts computation of the forces.
After this step each processor holds the {\em partial forces} for the 
available particles, {\eg} processor 0 holds forces acting on home particles
from processor 0, 1, 2 and 3. These forces must be accumulated and sent back
(right) to the home processor. Finally the update of the velocity and 
coordinates is done on the home processor. 

The {\tt communicate\_r} routine is given below in the full C-code:
\begin{footnotesize}
\begin{verbatim}
void communicate_r(int nprocs,int pid,rvec vecs[],int start[],int homenr[])
/* 
 * nprocs = number of processors
 * pid    = processor id (0..nprocs-1)
 * vecs   = vectors
 * start  = starting index in vecs for each processor
 * homenr = number of home particles for each processor
 */
{
  int i;        /* processor counter */
  int shift;    /* the amount of processors to communicate with */
  int cur;      /* current processor to send data from */
  int next;     /* next processor on a ring (using modulo) */

  cur   = pid;
  shift = nprocs/2;

  for (i=0; (i<shift); i++) {
    next=(cur+1) % nprocs;      
    send   (left,  vecs[start[cur]],  homenr[cur]);
    receive(right, vecs[start[next]], homenr[next]);
    cur=next;
  }
}
\end{verbatim}

\end{footnotesize}

The data flow around the ring is visualised in \figref{ring}. 
Note that because of the ring topology each processor automatically 
gets the proper particles to interact with.
\begin {figure}
\centerline{\psfig {figure=plots/ring.eps,width=6cm}}
\caption {Data flow in a ring of processors.}
\label{fig:ring}
\end {figure}

%\section {Shake}

%\begin {figure}
%\centerline{\psfig {figure=plots/shake.eps,height=15cm,width=8cm}}
%\caption {The Parallel Shake algorithm.}
%\label{fig:pshake}
%\end {figure}


