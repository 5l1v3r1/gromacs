\newcommand{\nproc}{\mbox{$M$}}
\newcommand{\natom}{\mbox{$N$}}

\section{Image calculation on a grid.}
\label{sec:nsgrid}
{\bf definition:} {\natom}: Number of particles, {\nproc} number of processors.\\
{\gromacs} uses an \myindex{interaction list} for nonbonded interactions,
usually called the {\em \myindex{neighbourlist}}.
This list is made every {\tt nstlist} MD steps, where {\tt nstlist} is
typically 10 MD steps. 
To make the neighbourlist all particles that are close 
(i.e. within the cut-off) to a given particle must be found.
This searching involves peridic boundary conditions, and 
determining the {\em image} (see Sec.~\ref{sec:pbc}).
When the cut-off is large compared to the box edge $l$ ($>$ 0.4$l$)
searching is done using an $O({\natom}^2)$ algorithm that computes
all distances and compares them to the cut-off.
When the cut-off is smaller than 0.4$l$ in all directions (x,y and z)
searching is done using a grid. All particles are put on a grid
with the smallest spacing  $>=$ 0.5 cut-off in each of the directions.
We have depicted the computational box, divided into grid-cells in 
Figure~\ref{fig:nsgrid}.
\begin{figure}[ht]
\centerline{\psfig{figure=plots/nsgrid.eps,width=6cm}}
\caption{The computational box in two dimensions, divided into grid-cells with three particles, $i$, $j$ and $k$. Each grid-cell is of size $>=$ 0.5 cut-off.}
\label{fig:nsgrid}
\end{figure}
In each spatial dimension, a particle $i$ has three images. For each direction
the image may be -1,0 or 1, corresponding to a translation over -1, 0 or +1
box vector. We do not search the surrounding grid-cells for
neighbours of $i$ and then calculate the image, 
but rather construct the images first and then 
search neighbours corresponding to that image of $i$.
Since we demand that the number of grid cells $>=$ 5 in each direction 
the same neighbour will not be found twice. For every particle, exactly 125 (5$^3$)
neighbouring cells are searched. Therefore, the algorithm scales linear with the 
number of particles. Although the prefactor is large (125) the scaling behaviour
makes the algorithm far superior over the standard $O({\natom}^2)$ 
algorithm when the 
number of particles exceeds a few hundred.

In the example of Figure~\ref{fig:nsgrid} the image $t_x$ = 0 of particle
$i$ will find $j$ as a neighbour, while image $t_x$ = 1 of particle $i$
will find $k$ as a neighbour.

\subsection{Domain decomposition for nonbonded forces}
For large parallel computers, domain decomposition is preferable over particle
decomposition, since it is easier to do load balancing. Without load balancing
the scaling of the code is rather poor...
For this purpose, the computational box is divided in {\nproc} slabs, where {\nproc}
is equal to the number of processors. There are multiple ways of dividing the box
over processors, but since the {\gromacs} code assumes
a ring topology for the processors, it is logical to cut the system in slabs in
just one dimension. For flexibility this dimension can be set by the user,
but it defaults to the X-dimension.
The algorithm for neighboursearching then becomes:
\begin{enumerate}
\item	Make a list of charge group indices sorted on (increasing) X coordinate
	(\figref{parsort}).
	{\bf Note} that care must be taken to parallelize the sorting algorithm
	as well. See \secref{parsort}.
\item	Divide this list into slabs, such that each slab has the same number of
	charge groups
\item	Put the particles corresponding to the local slab on a 3D grid as 
	desribed above (\secref{nsgrid})
\item	Communicate the grid to neighbouring processors (not necessarily to all
	processors). The amount of neighbouring grid-cells (N$_{gx}$) to 
	communicate is determined by the cut-off length $r_c$ according to
	\beq
	N_{gx}	~=~	\frac{r_c \nproc}{l_x}   
	\eeq
	where $l_x$ is the box length in the slabbing direction. 
\item	On each processor compute the neighbourlist for all charge groups in
	its slab using the normal grid neighboursearching.
\end{enumerate}

\begin{figure}[h]
\centerline{\psfig{figure=plots/parsort.eps,width=10cm}}
\caption{Index in the coordinate array. The division in slabs is indicated by dashed lines. Note that sorting can be done on X,Y or Z.}
\label{fig:parsort}
\end{figure}

For homogeneous system, this is close to an optimal load balancing, without 
actually doing load balancing. For inhomogeneous system, such as membranes, or 
interfaces, the dimension for slabbing must be chosen such that it is perpendicular
to the interface; in this fashion each processor has ``a little bit of everything''.

The following observations are important here:
\begin{itemize}
\item	Particles may diffuse from one slab to the other, therefore each processor
	must hold coordinates for all particles all the time, and distribute forces
	back to all processors as well.
\item	Velocities are kept on the ``home processor'' for each particle,
	where the integration of Newton's equations is done.
\item	Fixed interaction lists (bonds, angles etc.) are kept each
	on a single processor.
	Since all processors have all coordinates, it does not matter where
	interactions are calculated.
	The division is actually done by the {\gromacs} preprocessor 
	(\myindex{grompp}) and care is taken that, as far as possible,
	every processor gets the same number of bonded interactions.
\end{itemize}

In all, this makes for a mixed particle decomposition/domain decomposition scheme
for paralellization of the MD code. The communication costs are four times higher
than for the simple particle decomposition method described in \secref{par}
(the whole coordinate and force array are communicated accross the whole ring,
rather than half the array over half the ring).
However, for large numbers of processors the improved load balancing 
compensates this easily.

A further concern for the parallelization is the PPPM algorithm. This algorithm
works with a 3D Fast Fourier Transform.

\subsection{Parallel sorting}
\label{sec:parsort}
For the domain decomposition bit of {\gromacs} it is necessary to sort the 
coordinates (or rather the index to coordinates) every time a neighbourlist is made.
If we use brute force, and sort all coordinates on each processor (which is 
technically possible since we have all the coordinates), then this sorting procedure
will take a constant time (proportional to {\natom}$^2\log${\natom}, 
independent of the number of processors. We can however do a little
better, if we assume that particles diffuse only slowly.
A parallel sorting algorithm can be conceived as follows: \\
At the first step of the simulation
\begin{enumerate}
\item	Do a full sort of all indices using e.g. the  quicksort algorithm that is
	built-in in the standard C-library
\item	Divide the sorted array into slabs (as described above see 
	\figref{parsort}).
\end{enumerate}
At subsequent steps of the simulation:
\begin{enumerate}
\item	Send the indices for each processor to the preceeding processor (if
	not processor 0) and to the next processor (if not {\nproc}-1). The 
	communication associated with this operation is proportional to
	2{\natom}/{\nproc}.
\item	Sort the combined indices of the three (or two) processors. Note that
	the CPU time associated with sorting is now
	(3{\natom}/{\nproc})$^2\log$ (3{\natom}/{\nproc}).
\item	On each processor, the indices belonging to it's slab can be determined
	from the order of the array (\figref{parsort}).
\end{enumerate}

\section{Domain decomposition}
Modern day parallel computers, such as an IBM SP/2 or a
Cray T3E consist of relatively
small numbers of relatively fast scalar processors (typically 8 to 256).
The communication channels that are available in hardware on these machine are
not directly visible for the programmer, a software layer (like \myindex{MPI} or 
\myindex{PVM}) hides this, and makes communication from all processors to all
others possible. In contrast, in the {\gromacs} hardware~\cite{Berendsen95a}
only communication in a ring was available, i.e. each processor could communicate
with its direct neighbours only.

It seems logical to map the computational box of an MD simulation system 
to a 3D grid of 
processors (e.g. 4x4x4 for a 64 processor system). This ensures that most 
interactions that are local in space can be computed with information from 
neighbouring processors only. However, this means that there have to be
communication channels in 3 dimensions too, which is not necessarily the case.
Although this may be overcome in software, such a mapping is complicated for the MD
software as well, without clear benefits in terms of performance for most
parallel computers. 

Therefore we opt for a simple one-dimensional division scheme
for the computational box. Each processor gets a slab of this box in a user defined
dimension (i.e. X,Y or Z).
For the communication between processors this has two main advantages:
\begin{enumerate}
\item	Simplicity of coding. Communication can only be to two neigbours
	(called {\em left} and {\em right} in {\gromacs}).
\item	Communication can usually be done in large chunks, which makes it
	more efficient on most hardware platforms.
\end{enumerate}

Most interactions in molecular dynamics have in principle a short ranged character.
Bonds, angles and dihedrals are guaranteed to have the corresponding particles 
close in space.


