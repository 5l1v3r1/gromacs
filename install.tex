\chapter{Technical Details.}
\label{ch:install}
\section{Installation.}
The {\gromacs} code is distributed in SOURCE form by our WWW server at\\
{\wwwpage}\\
On this server you will find all the information you need to 
\normindex{install}
the software, as well as the \normindex{license form} that you have to submit
before you are allowed to down load the code. When you have filled in this
license form, a user name and password will be sent to you by e-mail
with which you can down load the files. The e-mail address you specify
on your license sheet will also be used to send you information on
updates, bug-fixes etc.

For \normindex{commercial use} of the software, please contact us
directly: {\email}

\section{Single or Double precision}
{\gromacs} can be compiled in both single\index{single precision}
and \normindex{double precision}.
Double precision will be 0 to 50\% slower than single precision depending
on the architecture you are running on. Double precision will use somewhat more
memory and run input, energy and full-precision trajectory files will be
almost twice as large.

The energies in single precision are accurate up to the last decimal,
the last one or two decimals of the forces are non-significant.
The virial is less accurate than the forces, since the virial is only one
order of magnitude larger than the size of each element in the sum over
all atoms (\secref{virial}).
In most cases this is not really a problem, since the fluctuations in de
virial can be 2 orders of magnitude larger than the average.
In periodic charged systems these errors are often negligible.
Especially cut-off's for the Coulomb interactions cause large errors
in the energies, forces and virial.
Even when using a reaction-field or lattice sum method the errors
are larger than or comparable to the errors due to the single precision.
Since MD is chaotic, trajectories with very similar starting conditions will
diverge rapidly, the divergence is faster in single precision than in double
precision.

For most simulations single precision is accurate enough.
In some cases double precision is required to get reasonable results:
\begin{itemize}
\item normal mode analysis,
for the conjugate gradient minimization and the calculation and
diagonalization of the Hessian
\item calculation of the constraint force between two large groups of atoms
\item energy conservation (this can only be done without temperature coupling
and without cut-off's)
\end{itemize}

\section{Porting {\gromacs}.}
The {\gromacs} system is designed with portability as one major design
goal. However there are a number of things we assume to be present on
the system {\gromacs} is being ported on. We assume the following
features:

\begin{enumerate}
\item 	the UNIX operating system (BSD 4.x or SYSTEM V rev.3 or higher) 
	or UNIX-like libraries
\item 	an ANSI C compiler 
\item	optionally a Fortran-77 compiler or Fortran-90 compiler
	for faster (on some computers) inner loop routines
\item	optionally an XDR library, which will allow you to use the
	portable versions of the {\gromacs} binary file types
	({\gromacs} files written in XDR format can be read on any
	architecture with a {\gromacs} version compiled with XDR)
\item 	If you want to use the graphics, the X-window system version 
	11 Release 4 or higher and the X-lib graphics libraries
\end{enumerate}

These are the requirements of a single processor system. If you want
to compile {\gromacs} on a multi processor environment there is another
requirement:

\begin{enumerate}
\item Message-passing architecture
\item Ring structure.
\end{enumerate}

One can understand that a message passing architecture also can be
mapped onto a shared memory machine. This implementation is left to
the reader as an exercise in parallel programming. Also the ring
structure can be mapped onto {\eg} a hypercube.

\subsection{Multi-processor Porting}

In the case you want to run the {\gromacs} software on a
multi-processor machine, you have two options.
\begin{enumerate}
\item	Install \normindex{MPI} or \normindex{PVM}. The {\gromacs} WWW
	page has some pointers to relevant documents.
\item	Write communication routines yourself. 
\end{enumerate}

It may be clear that you will hardly ever need to write the routines
yourself, but if you can't avoid it, here are some clues.
The interface between these routines and the
rest of the {\gromacs} system is described in the file {\tt
\$GMXHOME/src/include/network.h} We will give a short description of the
different routines below.

\begin{description}
\item[{\bf extern void gmx\_tx(int pid,void *buf,int bufsize);}]\mbox{}\\ 
This routine, when called with the destination processor number, a
pointer to a (byte oriented) transfer buffer, and the size of the
buffer will send the buffer to the indicated processor (in our case
always the neighboring processor). The routine does {\bf not} wait
until the transfer is finished.

\item[{\bf extern void gmx\_tx\_wait(int pid);}]\mbox{}\\
This routine waits until the previous, or the ongoing transmission is
finished.

\item[{\bf extern void gmx\_txs(int pid,void *buf,int bufsize);}]\mbox{}\\
This routine implements a synchronous send by calling the
a-synchronous routine and then the wait. It might come in handy to
code this differently.

\item[{\bf extern void gmx\_rx(int pid,void *buf,int bufsize);}]
\item[{\bf extern void gmx\_rx\_wait(int pid);}]\vspace{-\itemsep}
\item[{\bf extern void gmx\_rxs(int pid,void *buf,int bufsize);}]\vspace{-\itemsep}\mbox{}\\
The very same routines for receiving a buffer and waiting until the
reception is finished.

\item[{\bf extern void gmx\_init(int pid,int nprocs);}]\mbox{}\\
This routine initializes the different devices needed to do the
communication. In general it sets up the communication hardware (if it
is accessible) or does an initialize call to the lower level
communication subsystem.

\item[{\bf extern void gmx\_stat(FILE *fp,char *msg);}]\mbox{}\\
With this routine we can diagnose the ongoing communication. In the
current implementation it prints the various contents of the hardware
communication registers of the (\intel) multiprocessor boards to a
file.
\end{description}

\section{Environment Variables}
{\gromacs} programs may be influenced by the use of environment 
variables. First of all, the variables set in the \normindex{GMXRC} file
are essential for running and compiling {\gromacs}. Other variables are:
\begin{enumerate}
\item	DUMP\_NL, dump neighbor list. 
	If set to a positive number the {\em entire}
	neighbor list is printed in the log file (may be many megabytes).
	Mainly for debugging purposes, but may also be handy for
	porting to other platforms.
\item	IAMCOOL, when set prints cool quotes, otherwise
	your {\gromacs} life will be dull and boring.
\item	WHERE, when set print debugging info on line numbers.
\item	LOG\_BUFS, the size of the buffer for file I/O. When set
	to 0, all file I/O will be unbuffered and therefore very slow.
	This can be handy for debugging purposes, because it ensures
	that all files are always totally up-to-date.
\item   GMXNPRI, for SGI systems only. When set, gives the
	default non-degrading priority (npri) for {\tt
	mdrun}, {\tt nmrun}, {\tt g\_covar} and {\tt g\_nmeig},
	{\eg}\@ setting \verb'setenv GMXNPRI 250' causes all
	runs to be performed at near-lowest priority by default.
\end{enumerate}

Some other environment variables are specific to one program, such as
TOTAL for the {\tt \normindex{do\_shift}} program, and DSPP for the
{\tt \normindex{do\_dssp}} program.
