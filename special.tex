\chapter{Special Topics}
\label{ch:special}
\section{\normindex{NMR} refinement}
The {\gromacs} software can be used to do NMR structure refinements.
A list of distance restrains based on NOE data can be added in your
topology file, like in the following example:
\begin{verbatim}
[ distance_restraints ]
; ai    aj      type    index   type'   low      up1     up2     fac
10      16      1       0       1       0.0      0.3     0.4     1.0 
10      28      1       1       1       0.0      0.3     0.4     1.0 
10      46      1       1       1       0.0      0.3     0.4     1.0 
16      22      1       2       1       0.0      0.3     0.4     2.5 
16      34      1       3       1       0.0      0.5     0.6     1.0 
\end{verbatim}
In this example a number of features can be found.
In columns {\tt ai} and {\tt aj} you find the atom numbers of the
particles to be restrained. The {\tt type} column should always be 1.
As explained in ~\secref{disre}, multiple distances can add
to a single NOE signal. In the topology this can be set using the
{\tt index} column. In our example, the restraints 10-28 and 10-46
both have index 1, therefore they are treated simultaneously.
An extra requirement for treating restraints together, is that 
the restraints should be on successive lines, without any
other intervening restraint. The columns {\tt up1} and {\tt up2} hold
the values of $r_1$ and $r_2$ from ~\eqnref{disre}.
In some cases in can be useful to have different force constants for
some restraints, this is controlled by the column {\tt fac}.
The force constant in the parameter file is multiplied by the value in the
column {\tt fac} for each restraint.
The columns {\tt type'} and {\tt low} hold variables, which are as of yet 
not used. They must be specified in the topology file however.

Some paramerts for NMR refinement using can be specified in the
{\tt grompp.mdp} file:
\begin{description}
\item[{\tt disre}: type of distance restraining.]
	The {\tt disre} variable sets the type of distance restraining.
	{\tt no/simple} turns the distance restraining off/on.
 	When multiple proteins or peptides are used
	in the simulation ensemble averaging 
	can be turned on by setting {\tt disre = ensemble}.
\item[{\tt disre\_weighting}: force-weighting in restraints with
	 multiple pairs.]
	The distance restraint force can be distributed equally
	over all the pairs involved in the restraint by setting
	{\tt disre\_weighting = equal}.
	The option {\tt disre\_weighting = conservative}
	gives conservative forces when {\tt disre\_tau = 0}.
	See ~\secref{disre} for details.
\item[{\tt disre\_mixed}: how to calculate the violations.]
	{\tt disre\_mixed = no} gives normal running time
	averaged violations.
	When {\tt disre\_mixed = yes} the square root of the
	product of the running time averaged and the instantaneous
	violations is used.
	See ~\secref{disre} for details.
\item[{\tt disre\_fc}: force constant $k_{dr}$ for distance restraints.] 
	$k_{dr}$  (\eqnref{disre}) can be set
	as variable {\tt disre\_fc = 1000} for a force constant of
	1000 {kJ mole$^{-1}$ nm$^{-2}$}. This value is multiplied by
	the value in the {\tt fac} column in the distance restraint
	entries in the topology file.
\item[{\tt disre\_tau}: time constant for restraints.] 
	$\tau$ (\eqnref{ravdisre}) can be set
	as variable {\tt disre\_tau = 10} for a time constant of
	10 ps.
\item[{\tt nstdisreout}: pair distance output frequency.]
	Determines how often the running time averaged and 
	instantaneous distances of all atom pairs involved in
	distance restraints are written to the energy file.
\end{description}



\section{Running with PVM.}
If you have a parallel computer, it may be equipped
with PVM (Parallel Virtual Machines, see also chapter~\ref{ch:algorithms}),
otherwise, have your system administrator install it. The package
is public domain software and supports virtually every commercially
available computer, such as an SGI Power Challenge, Paragon {\intel} box,
Thinking machines CM-5, CRAY-J9036287, Convex MPP, etc., or on a cluster
of workstations.

The {\gromacs} software can work with the PVM library, but only
on computers with the same processor, it is not possible to mix
eg. Sparc and MIPS chips. We will assume
here that the software is installed with PVM. A sample PVM session
is described below.

First, set the PVM environment variables in your {\tt .cshrc} file.
\begin{verbatim}
setenv	PVM_ROOT=/home/pvm
setenv	PVM_ARG=SGI
\end{verbatim}
You also need access to a number of workstations, let's call them
{\bf vince}, {\bf butch} and {\bf mia}, we'll assume your username
is {\bf wallace}. Make a {\tt .rhosts} file in your home directory:
\begin{verbatim}
vince   wallace
butch   wallace
mia     wallace
\end{verbatim}
Now log off and on again to effectuate all this (assuming you are sitting on 
{\tt vince}). Start the pvm front-end: 
\begin{verbatim}
% pvm
pvm>add butch mia
2 successful
                    HOST     DTID
                   vince    80000
                     mia   100000
pvm>quit

pvmd still running.
%
\end{verbatim}
Now you can use {\gromacs} with pvm. You just have to add the option
{\tt -N 3} to your {\tt grompp} and {\tt mdrun} command lines. Since the
remotely running mdruns will start from your home directory, give a full 
path for the log file, eg.: {\tt -g /data/pulp/wallace/speptide/md}.

PVM jobs can be stopped within the pvm command line utility with
{\tt kill process}. All pvms can be terminated with the {\tt halt} command.

\section{Running with MPI}
If you have installed the MPI (Message Passing Interface) on your computer(s)
you can compile {\gromacs} with this communication library. Some
hardware vendors provide optimized MPI libraries for shared-memory
architectures, or whatever is fast on their particular platform.
Compiling the {\gromacs} distribution with MPI support is straightforward.
Edit your {\tt Makefile.\$CPU} in the {\tt gmxhome/src/makef} directory,
and set the {\tt USE\_MPI} variable to {\tt yes} and recompile all sources.
If all is well, you can now run with MPI. 

There usually is a program called {\tt mpirun} with which you can fire
up the parallel processes. A typical command line looks like:
\type{mpirun -p goofus,doofus,fred 10 mdrun -s topol -v -N 30}
this runs on each of the machines goofus,doofus,fred with 10 processes
on each\footnote{This example taken from Silicon Graphics manual}.

If you have a single machine with multiple processors you don't have to
use the {\tt mpirun} command, but you can do with an extra option to
{\tt mdrun}:
\type{mdrun -np 8 -s topol -v -N 8}
In this example MPI reads the first option from the command line.
Since {\tt mdrun} also wants to know the number of processes you have to
type it twice.
Please note that no automatic nicing is done, which means that only
the first process will be niced by default. 
Check your local manuals (or online manual) for exact details
of your MPI implementation.

The online manual for MPI on the web can be found at:\\
{\tt http:://www.mcs.anl.gov/mpi/index.html}
