\chapter{Special Topics}
\label{ch:special}
\section{NMR refinement}
The {\gromacs} software can be used to do NMR structure refinements.
A list of distance restrains based on NOE data can be added in your
topology file, like in the following example:
\begin{verbatim}
[ distance_restraints ]
; ai    aj      type    index   rt-aver rx0      rx1     rx2     rx3
10      16      1       0       1       0.3      0.4     0.0     0.0 
10      28      1       1       1       0.3      0.4     0.0     0.0 
10      46      1       1       1       0.3      0.4     0.0     0.0 
16      22      1       2       1       0.3      0.4     0.0     0.0 
16      34      1       3       1       0.5      0.6     0.0     0.0 
\end{verbatim}
In this example a number of features can be found.
In columns {\tt ai} and {\tt aj} you find the atom numbers of the
particles to be restrained. The {\tt type} column should always be 1.
As explained in ~\secref{disre}, multiple distances can add
to a single NOE signal. In the topology this can be set using the
{\tt index} column. In our example, the restraints 10-28 and 10-46
both have index 1, therefore they are treated simultaneously.
An extra requirement for treating restraints together, is that 
the restraints should be on successive lines, without any
other intervening restraint. The columns {\tt rx0} and {\tt rx1} hold
the values of $r_0$ and $r_1$ from ~\eqnref{disre}.
The columns {\tt rx2} and {\tt rx3} hold variables, which are as of yet 
not used. They must be specified in the topology file however.

Some more parameters are important for NMR refinement using
distance restraints in {\gromacs}.
\begin{description}
\item[Force constant $k_{dr}$ for distance restraints.] 
	$k_{dr}$  (\eqnref{disre}) can be set in the {\tt grompp.mdp} file
	as variable {\tt disre\_fc = 1000} for a force constant of
	1000 {kJ mole$^{-1}$ nm$^{-2}$}.
\item[Time constant $\tau$ for restraints.] 
	$\tau$ (\eqnref{ravdisre}) can be set in the {\tt grompp.mdp} file
	as variable {\tt disre\_tau = 10} for a time constant of
	10 ps.
\item[Ensemble averaging.] When multiple proteins or peptides are used
	in the simulation ensemble averaging 
	can be turned on by passing the option
	{\tt -ensemble} to the {\gromacs} preprocessor {\tt grompp}.
\end{description}



\section{Running with PVM.}
If you have a parallel computer, it may be equipped
with PVM (Parallel Virtual Machines, see also chapter~\ref{ch:algorithms}),
otherwise, have your system administrator install it. The package
is public domain software and supports virtually every commercially
available computer, such as an SGI Power Challenge, Paragon {\intel} box,
Thinking machines CM-5, CRAY-J9036287, Convex MPP, etc., or on a cluster
of workstations.

The {\gromacs} software can work with the PVM library, but only
on computers with the same processor, it is not possible to mix
eg. Sparc and MIPS chips. We will assume
here that the software is installed with PVM. A sample PVM session
is described below.

First, set the PVM environment variables in your {\tt .cshrc} file.
\begin{verbatim}
setenv	PVM_ROOT=/home/pvm
setenv	PVM_ARG=SGI
\end{verbatim}
You also need access to a number of workstations, let's call them
{\bf vince}, {\bf butch} and {\bf mia}, we'll assume your username
is {\bf wallace}. Make a {\tt .rhosts} file in your home directory:
\begin{verbatim}
vince   wallace
butch   wallace
mia     wallace
\end{verbatim}
Now log off and on again to effectuate all this (assuming you are sitting on 
{\tt vince}). Start the pvm front-end: 
\begin{verbatim}
% pvm
pvm>add butch mia
2 successful
                    HOST     DTID
                   vince    80000
                     mia   100000
pvm>quit

pvmd still running.
%
\end{verbatim}
Now you can use {\gromacs} with pvm. You just have to add the option
{\tt -N 3} to your {\tt grompp} and {\tt mdrun} command lines. Since the
remotely running mdruns will start from your home directory, give a full 
path for the log file, eg.: {\tt -g /data/pulp/wallace/speptide/md}.

PVM jobs can be stopped within the pvm command line utility with
{\tt kill process}. All pvms can be terminated with the {\tt halt} command.

\section{Running with MPI}
If you have installed the MPI (Message Passing Interface) on your computer(s)
you can compile {\gromacs} with this communication library. Some
hardware vendors provide optimized MPI libraries for shared-memory
architectures, or whatever is fast on their particular platform.
Compiling the {\gromacs} distribution with MPI support is straightforward.
Edit your {\tt Makefile.\$CPU} in the {\tt gmxhome/src/makef} directory,
and set the {\tt USE\_MPI} variable to {\tt yes} and recompile all sources.
If all is well, you can now run with MPI. 

There usually is a program called {\tt mpirun} with which you can fire
up the parallel processes. A typical command line looks like:
\type{mpirun -p goofus,doofus,fred 10 mdrun -s topol -v -N 30}
this runs on each of the machines goofus,doofus,fred with 10 processes
on each\footnote{This example taken from Silicon Graphics manual}.

If you have a single machine with multiple processors you don't have to
use the {\tt mpirun} command, but you can do with an extra option to
{\tt mdrun}:
\type{mdrun -np 8 -s topol -v -N 8}
In this example MPI reads the first option from the command line.
Since {\tt mdrun} also wants to know the number of processes you have to
type it twice.
Please note that no automatic nicing is done, which means that only
the first process will be niced by default. 
Check your local manuals (or online manual) for exact details
of your MPI implementation.

The online manual for MPI on the web can be found at:\\
{\tt http:://www.mcs.anl.gov/mpi/index.html}
